import os
from typing import List
import streamlit as st
from langchain.vectorstores import Pinecone
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import Document
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough, RunnableParallel, RunnableLambda
import pinecone

# API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = st.secrets["openai"]["api_key"]
os.environ["PINECONE_API_KEY"] = st.secrets["pinecone"]["api_key"]

# Streamlit UI ì„¤ì •
st.header("Chat with the Conference 2022-2024 ğŸ’¬ ğŸ“š")
option = st.selectbox("GPT ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.", ("gpt-4", "gpt-3.5-turbo"))
llm = ChatOpenAI(model=option)

# Pinecone ì´ˆê¸°í™”
pinecone.init(api_key=st.secrets["pinecone"]["api_key"], environment=st.secrets["pinecone"]["environment"])

# Pinecone ì¸ë±ìŠ¤ ì„¤ì •
index_name = "gtc2024"
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
vectorstore = Pinecone.from_existing_index(index_name, embeddings)
retriever = vectorstore.as_retriever(
    search_type='mmr',
    search_kwargs={"k": 5, "fetch_k": 10, "lambda_mult": 0.75}
)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
template = """
You are a Korean assistant for question-answering tasks. 
Use the following pieces of retrieved context to answer the question. 
If you don't know the answer, just say that you don't know. 
You should answer in KOREAN and please give rich sentences to make the answer much better.

Question: {question} 
Context: {context} 
Answer:
"""

prompt = ChatPromptTemplate.from_template(template)

def format_docs(docs: List[Document]) -> str:
    """Convert Documents to a single string."""
    formatted = [
        f"Article Title: {doc.metadata['source']}\nArticle Snippet: {doc.page_content}"
        for doc in docs
    ]
    return "\n\n" + "\n\n".join(formatted)

# RunnableLambda for formatting docs
format_docs_lambda = RunnableLambda(lambda docs: format_docs(docs))

# Answer generation chain
answer = prompt | llm | StrOutputParser()

chain = (
    RunnableParallel(question=RunnablePassthrough(), docs=retriever)
    .assign(context=format_docs_lambda)
    .assign(answer=answer)
    .pick(["answer", "docs"])
)

# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
if "messages" not in st.session_state.keys():  # Initialize the chat message history
    st.session_state.messages = [
        {"role": "assistant", "content": "Conferenceì—ì„œ ê³µê°œëœ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”!"}
    ]

if prompt_message := st.chat_input("Your question"): 
    st.session_state.messages.append({"role": "user", "content": prompt_message})

for message in st.session_state.messages: 
    with st.chat_message(message["role"]):
        st.write(message["content"])

if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = chain.invoke({"question": prompt_message})
            answer = response['answer']
            source_documents = response['docs']
            st.markdown(answer)

            with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                for i, doc in enumerate(source_documents[:3], 1):
                    st.markdown(f"{i}. {doc.metadata['source']}", help=doc.page_content)
            message = {"role": "assistant", "content": response['answer']}
            st.session_state.messages.append(message)
